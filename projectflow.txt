\-------------------------Setting up project structure---------------------------

1. Create repo, clone it in local
2. Create a virtual environment named **"atlas"**

   ```bash
   conda create -n atlas python=3.10
   conda activate atlas
   ```
3. Install the cookiecutter data‑science template

   ```bash
   pip install cookiecutter
   cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science
   ```
4. Rename `src.models` ➜ `src.model`
5. `git add` → `commit` → `push`

\-------------------------Set up MLflow on DagsHub---------------------------
6\. Log in to **[https://dagshub.com/dashboard](https://dagshub.com/dashboard)**
7\. **Create > New Repo > Connect a repo > (GitHub)** → select your repo → **Connect**
8\. Copy the experiment‑tracking URL & code snippet (or **Go to MLflow UI**).
9\. \`\`\`bash
pip install dagshub mlflow

````
10. Run the experiment notebooks, then `git add → commit → push`.

-------------------------Data Version Control (DVC)---------------------------
11. ```bash
dvc init
mkdir local_s3            # temporary local cache
dvc remote add -d mylocal local_s3
````

12. Add modular code inside **src**:

* `logger/`
* `data_ingestion.py`
* `data_preprocessing.py`
* `feature_engineering.py`
* `model_building.py`
* `model_evaluation.py`
* `register_model.py`

13. Create `dvc.yaml` (pipeline up to *model\_evaluation.metrics*) & `params.yaml`.
14. Run pipeline & check status

```bash
dvc repro
dvc status
```

15. `git add → commit → push`

\-------------------------Object Storage (S3‑compatible)---------------------------
16\. Create a **DigitalOcean Space** (acts like an S3 bucket) and an **access key/secret**.
17\. \`\`\`bash
pip install "dvc\[s3]"     # S3 compatibility
export AWS\_ACCESS\_KEY\_ID=...       # DO Spaces key
export AWS\_SECRET\_ACCESS\_KEY=...
export AWS\_DEFAULT\_REGION="us-east-1"   # any region string works

# add DO Space as remote

dvc remote add -d myremote s3://\<space‑name>/\<optional‑prefix>
dvc push

````

-------------------------API Packaging---------------------------
18. `mkdir flask_app` and add API code & assets.
19. ```bash
pip install flask
````

20. Local test: `python app.py`    ⚙️ (push data to Space with `dvc push` if updated)

\-------------------------CI pipeline scaffold---------------------------
21\. \`\`\`bash
pip freeze > requirements.txt
mkdir -p .github/workflows

# add ci.yaml (build, test, push image, deploy)

````
22. **Generate a DagsHub token** → _Settings > Tokens > Generate_ → save as **dagshub_key**.
   Add this as a GitHub repository secret & reference it in `ci.yaml`.
23. Add `tests/` and `scripts/` for CI checks.


======================== Moving to Docker ========================
24. ```bash
pip install pipreqs
cd flask_app
pipreqs . --force
````

25. Add a **Dockerfile**, start Docker Desktop, then build & test locally:

```bash
docker build -t cricket-app:latest .
docker run -p 8888:5000 -e CAPSTONE_TEST=$dagshub_key cricket-app:latest
```

(Optional) Push to Docker Hub or DigitalOcean Container Registry (DOCR).

\-------------------------DigitalOcean Credentials---------------------------
26\. In GitHub Secrets & Variables store:

* `DIGITALOCEAN_ACCESS_TOKEN`
* `DOCKERHUB_USERNAME` & `DOCKERHUB_PASSWORD` **or** `DOCR_NAME`
* `DAGSHUB_KEY` (capstone\_test)
* any other env vars your app needs

27. Configure **ci.yaml** stages:
28. Build & test Docker image.
29. Push image to Docker Hub/DOCR.
30. Deploy (kubectl apply) to DigitalOcean Kubernetes via `doctl`.

---

## \*\*\*\*\*\*\*\*\* Setup before DigitalOcean Kubernetes (DOKS) deployment \*\*\*\*\*\*\*\*\*

◆ **Install DigitalOcean CLI**

```bash
# macOS/Homebrew
brew install doctl
# Windows (PowerShell)
choco install doctl
```

◆ **Authenticate & verify**

```bash
doctl auth init --access-token $DIGITALOCEAN_ACCESS_TOKEN
doctl account get
```

◆ **Install kubectl** (if not already)

```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/windows/amd64/kubectl.exe"
# move kubectl.exe to a folder in PATH (e.g., C:\Windows\System32)
```

◆ **Create a DOKS cluster**

```bash
doctl kubernetes cluster create flask-app-cluster \
    --region syd1 \
    --version 1.30.0 \
    --size s-2vcpu-4gb \
    --node-pool "name=flask-app-nodes;count=1;size=s-2vcpu-4gb"
```

◆ **Fetch kubeconfig & check nodes**

```bash
doctl kubernetes cluster kubeconfig save flask-app-cluster
kubectl get nodes
kubectl get namespaces
```

◆ **Deploy the app**

```bash
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml   # LoadBalancer
```

◆ **Retrieve external IP & test**

```bash
kubectl get svc flask-app-service
curl http://<external-ip>:5000
```

---

> > > > > > > > > >  Prometheus Server Setup (on DigitalOcean Droplet) <<<<<<<<<<

---

28. **Create an Ubuntu Droplet** for Prometheus (t3.medium ≈ 2 vCPU/4 GB) with ports 9090 (HTTP) & 22 (SSH) open.
29. SSH in & update:

```bash
ssh root@<droplet-ip>
apt update && apt upgrade -y
```

30. Install Prometheus:

```bash
useradd --no-create-home prometheus
mkdir /etc/prometheus /var/lib/prometheus
curl -LO https://github.com/prometheus/prometheus/releases/download/v2.46.0/prometheus-2.46.0.linux-amd64.tar.gz
 tar -xzf prometheus-2.46.0.linux-amd64.tar.gz
 mv prometheus-2.46.0.linux-amd64/prometheus /usr/local/bin/
 mv prometheus-2.46.0.linux-amd64/promtool /usr/local/bin/
 mv prometheus-2.46.0.linux-amd64/consoles /etc/prometheus
 mv prometheus-2.46.0.linux-amd64/console_libraries /etc/prometheus
```

31. **Configure Prometheus** `/etc/prometheus/prometheus.yml`:

```
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: "flask-app"
    static_configs:
      - targets: ["<external‑ip‑of‑k8s‑service>:5000"]
```

32. Create a systemd service & start Prometheus.
    Verify at `http://<droplet-ip>:9090`.

---

> > > > > > > > > >  Grafana Server Setup (on DigitalOcean Droplet) <<<<<<<<<<

---

33. Create a second Ubuntu Droplet (or reuse the Prometheus one) with port 3000 open.
34. SSH & install Grafana:

```bash
apt update && apt upgrade -y
wget https://dl.grafana.com/oss/release/grafana_10.1.5_amd64.deb
apt install ./grafana_10.1.5_amd64.deb -y
systemctl start grafana-server
systemctl enable grafana-server
```

35. Access **http\://<droplet-ip>:3000** (admin / admin).
36. **Add Prometheus data source** (`http://<prometheus-droplet-ip>:9090`) and build dashboards.

---

## AWS‑specific Cleanup ➜ DigitalOcean Cleanup

• **Delete K8s resources**

```bash
kubectl delete deployment flask-app
kubectl delete service flask-app-service
kubectl delete secret capstone-secret
```

• **Destroy the DOKS cluster**

```bash
doctl kubernetes cluster delete flask-app-cluster
```

• **Destroy Droplets** for Prometheus/Grafana via control panel or `doctl compute droplet delete`.
• **Delete DO Spaces** objects if no longer needed.
• **Prune registry images** (Docker Hub or DOCR).

---

What is a PVC?
A **PersistentVolumeClaim (PVC)** is a Kubernetes request for storage. It binds to a **PersistentVolume (PV)**—the actual disk resource. PVCs reference a **StorageClass** which defines *how* the volume is provisioned (e.g., DigitalOcean Block Storage, NFS). When a pod requires storage it mounts the volume claimed via the PVC.


disable app: kubectl scale deployment flask-app --replicas=0 -n default  
enable app: kubectl scale deployment flask-app --replicas=2 -n default  